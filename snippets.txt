Explaining methodology
===
Each RQ is subdivided into two parts: investigating the actual effect (i.e. do posts that link to Wikipedia articles have higher scores) and investigating whether there is a causal relationship. Therefore, in each case we select measurements relevant to the RQ, compare the descriptive stats, and then perform a causal analysis with two different models. For causal analysis, we use a propensity score stratification approach (first described in [CITE] and used in the context of HCI in various cases [a, b, c, d]) in addition to a covariate nearest neighbors matching approach.
We believe the platforms we study are particularly well-suited to the use of propensity scores and covariate matching because the platform are extremely transparent - we have access to almost all information about user behavior on the platform. This allows us to make a plausible case for causality, as our modeling approach addresses most plausible confounders. We discuss this in greater detail below, and also offer some discussion about possible confounders that were not accounted for by our model.

RQ1 and RQ2 use the same output metrics. To answer these questions, we are interested in measurements that correspond to the value a single post adds to the platform. We identify two distinct parts of the platform that define value differently. The private companies that own the platforms receive economic value through increased revenue and increased opportunities for revenue and funding (i.e. pageviews lead to additional advertising revenue, and increased user engagement can lead to better advertising prices or better investment opportunities). <BAD> The users of the platform receive value from content in the form of better consumable content (i.e. programming answers on SO and various media on reddit).

In answering RQ1 and RQ2 we focus on the following metrics: the score (equal to upvotes minus downvotes) that an item receives, the number of comments that an item receives, and the total number of items that appear on the platform. Since Stack Overflow provides access to pageview data (last updated May 2, 2017 [cite BigQuery]), we also use the pageviews that an item receives as an output metric. This is the metric that lets us make the most accurate economic estimations "how much of Stack Overflow's revenue is related to Wikipedia content" and "how much of Stack Overflow's revenue is likely caused by Wikipedia content". SO is relatively open about its business model (various SO employees have written blog posts, and the CEO even answers questions occasionally online). On reddit, score is used to estimate the advertising revenue that a given item contributes.

RQ1 is concerned with how the presence of Wikipedia articles affects content quality on external platforms, whereas RQ2 is concerned with how the quality of articles affects content quality on external platforms. For RQ1, we separate all items in our random sample into a control and treatment groups. The treatment group consists of all items (SO answers and reddit posts) that include a valid (tested via Python script) link to a Wikipedia article, such that users could actually follow the link and view a Wikipedia article. The "control" group consisted of all other items. After randomly sampling X rows from reddit (Y% of all items), the reddit treatment group consisted of X items and the reddit control group consisted of Y items. Likewise, after randomly X items from SO (Y% of all items), the SO treatment group had X items and the SO control group had Y items.

For RQ2, we restrict analysis to items that include a valid Wikipedia link (the control group for RQ1). To get a measurement of quality, we made use of the WikiMedia Research Foundation's Artifical Intelligence quality prediction tool, ORES [cite]. ORES is a publicly accessible API that examines a specific revision of a Wikipedia article, and computes the most likely classification of quality. These classifications are based on Wikipedia's internal quality rating system; articles are classified as one of the following: "Stub", "Start", "C", "B", "Good Article", "Featured Article". While the ORES tool has been showing to achieve very high accuracy [cite] and has already been utilized for vandalism protection [cite], there are some issues with this approach. "Quality" of a text corpus is inherently subjective [cite?]. Additionally, there is no guarantee that a "Featured Article" is better than a "Good Article" - the Featured Article designation does not always correspond to quality [cite, ask Brent]. Therefore, the classifications from ORES do not result in a monotonic scale of quality that would be useful for a linear regression. To address these issues, we take a binary approach to classifying "high quality" Wikipedia articles. Any article that ORES predicts to be a a "Good Article" or a "Featured Article" is included in the "high quality" treatment group for RQ2. Any other that ORES predicts to be a "Stub", "Start", "C", or "B" article (i.e. all other articles that ORES can successfully analyze) is included in the control group. Some items had a article revision that could not be analyzed by ORES - these items were not included in either group.

Finally, to answer RQ3, we identify new output metrics. With RQ3, we are interested in measurements of how Wikipedia editing behavior changed in response to Wikipedia links being posted on external sites. The simplest measurement to answer this question is the change in number of edits that were made to a given article. Edit count is not necessarily a strong indicator of increased quality - Wikipedia even has an article detailing why edit count is a poor measurement of user contribution to the community [cite]. Therefore, we examine the relative change in edit count (number of edits made 1 week after posting divided by number of edits made 1 week before posting). This means that "hot" articles that already have a large number of edits made regularly will not overly influence results. Likewise, "finished" articles that are relatively stagnant will not also not exert undue influence on the results.

While the relative change in edit count is a reasonable way to approximate the general effect that SO/reddit posts have on Wikipedia edit behavior, we also seek to investigate what type of edit behavior is occurring. This is possible to investigate because the MediaWiki api allows us to identify the age of editor accounts, the number of edits made by each editor, and whether a given edit was a "minor" edit. Using account age and edit count, we classify editors into 3 classifications: brand new users (created after the link was posted), active existing users (user created before the link was posted, with more than 5 edits), and inactive existing users (user created before the link was posted, but with 5 or less total edits). For each classification, we can calculate the change in editing behavior for the subgroup, and therefore gain insight into what type of behavior change occurred. Additionally, we can see if link posts are promoting minor edits or major edits. Finally, we also check for two specific phenomena that we speculated may occur. First, are there any cases where a Wikipedia article is heavily edited immediately before it is posted (implying that users may be using Wikipedia as a sort of malleable citation source to support a point or arguement)? Second, do we observe an increase in quality (implying a symbiotic relationship between the UGC platforms)?


The results for each RQ are presented below. Summarized key results appear in Table X.