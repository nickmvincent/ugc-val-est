Explaining methodology
===
Each RQ is subdivided into two parts: investigating the actual effect (i.e. do posts that link to Wikipedia articles have higher scores) and investigating whether there is a causal relationship. Therefore, in each case we select measurements relevant to the RQ, compare the descriptive stats, and then perform a causal analysis with two different models. For causal analysis, we use a propensity score stratification approach (first described in [CITE] and used in the context of HCI in various cases [a, b, c, d]) in addition to a covariate nearest neighbors matching approach.
We believe the platforms we study are particularly well-suited to the use of propensity scores and covariate matching because the platform are extremely transparent - we have access to almost all information about user behavior on the platform. This allows us to make a plausible case for causality, as our modeling approach addresses most plausible confounders. We discuss this in greater detail below, and also offer some discussion about possible confounders that were not accounted for by our model.

RQ1 and RQ2 use the same output metrics. To answer these questions, we are interested in measurements that correspond to the value a single post adds to the platform. We identify two distinct parts of the platform that define value differently. The private companies that own the platforms receive economic value through increased revenue and increased opportunities for revenue and funding (i.e. pageviews lead to additional advertising revenue, and increased user engagement can lead to better advertising prices or better investment opportunities). <BAD> The users of the platform receive value from content in the form of better consumable content (i.e. programming answers on SO and various media on reddit).

In answering RQ1 and RQ2 we focus on the following metrics: the score (equal to upvotes minus downvotes) that an item receives, the number of comments that an item receives, and two metrics related to the prevalence of Wikipedia links: the percentage of items that include Wikipedia links and how frequently Wikipedia links appear compared to links to other domains. Since Stack Overflow provides access to pageview data (last updated May 2, 2017 [cite BigQuery]), we also use the pageviews that an item receives as an output metric. This is the metric that lets us make the most accurate economic estimations "how much of Stack Overflow's revenue is related to Wikipedia content" and "how much of Stack Overflow's revenue is likely caused by Wikipedia content". SO is relatively open about its business model (various SO employees have written blog posts, and the CEO even answers questions occasionally online). On reddit, score is used to estimate the advertising revenue that a given item contributes.

RQ1 is concerned with how the presence of Wikipedia articles affects content quality on external platforms, whereas RQ2 is concerned with how the quality of articles affects content quality on external platforms. For RQ1, we separate all items in our random sample into a control and treatment groups. The treatment group consists of all items (SO answers and reddit posts) that include a valid (tested via Python script) link to a Wikipedia article, such that users could actually follow the link and view a Wikipedia article. The "control" group consisted of all other items. After randomly sampling X rows from reddit (Y% of all items), the reddit treatment group consisted of X items and the reddit control group consisted of Y items. Likewise, after randomly X items from SO (Y% of all items), the SO treatment group had X items and the SO control group had Y items.

For RQ2, we restrict analysis to items that include a valid Wikipedia link (the control group for RQ1). To get a measurement of quality, we made use of the WikiMedia Research Foundation's Artifical Intelligence quality prediction tool, ORES [cite]. ORES is a publicly accessible API that examines a specific revision of a Wikipedia article, and computes the most likely classification of quality. These classifications are based on Wikipedia's internal quality rating system; articles are classified as one of the following: "Stub", "Start", "C", "B", "Good Article", "Featured Article". While the ORES tool has been showing to achieve very high accuracy [cite] and has already been utilized for vandalism protection [cite], there are some issues with this approach. "Quality" of a text corpus is inherently subjective [cite?]. Additionally, there is no guarantee that a "Featured Article" is better than a "Good Article" - the Featured Article designation does not always correspond to quality [cite, ask Brent]. Therefore, the classifications from ORES do not result in a monotonic scale of quality that would be useful for a linear regression. To address these issues, we take a binary approach to classifying "high quality" Wikipedia articles (items either have "good" or "bad" links on average). To identify items with "good" links, we convert the ORES quality classifications into a simple numerical scale (Stub=0, Start=1, C=2, B=3, GA=4, FA=5). After getting the score for all links in a single item (for reddit posts, there is always just one link, but an SO post might have many links), we average the score. If the average score is greater than or equal to 4, the item is included in the treatment group. Thus the treatment group represents all posts that, on average, include "good" Wikipedia articles. Some items had a article revision that could not be analyzed by ORES - these items were not included in either group ($VERIFY). 


Finally, to answer RQ3, we identify new output metrics. With RQ3, we are interested in measurements of how Wikipedia editing behavior changed in response to Wikipedia links being posted on external sites. The simplest measurement to answer this question is the change in number of edits that were made to a given article. Edit count is not necessarily a strong indicator of increased quality - Wikipedia even has an article detailing why edit count is a poor measurement of user contribution to the community [cite]. Therefore, we examine the relative change in edit count (number of edits made 1 week after posting divided by number of edits made 1 week before posting). This means that "hot" articles that already have a large number of edits made regularly will not overly influence results. Likewise, "finished" articles that are relatively stagnant will not also not exert undue influence on the results.

While the relative change in edit count is a reasonable way to approximate the general effect that SO/reddit posts have on Wikipedia edit behavior, we also seek to investigate what type of edit behavior is occurring. This is possible to investigate because the MediaWiki api allows us to identify the age of editor accounts, the number of edits made by each editor, and whether a given edit was a "minor" edit. Using account age and edit count, we classify editors into 3 classifications: brand new users (created after the link was posted), active existing users (user created before the link was posted, with more than 5 edits), and inactive existing users (user created before the link was posted, but with 5 or less total edits). For each classification, we can calculate the change in editing behavior for the subgroup, and therefore gain insight into what type of behavior change occurred (i.e. before a link was posted 90% of edits were made by active users, but afterwards only %80 of posts were made by active users). Additionally, we can see if link posts are promoting minor edits or major edits. Finally, we also check for two specific phenomena that we speculated may occur. First, are there any cases where a Wikipedia article is heavily edited immediately before it is posted (implying that users may be using Wikipedia as a sort of malleable citation source to support a point or argument)? Second, do we observe an increase in quality (implying a symbiotic relationship between the UGC platforms)?

Summarized key results appear in Table X. When discussing differences in means between groups, all differences were determined to be statistically significant by unpaired, independent t-test, implemented by scipy [cite]. Exact p-values appear in Table X.

The first two metrics we observe help explain the prevalance of Wikipedia links on the platform. The first "prevalence metric" is the percent of items that include a Wikipedia link. This metric is used to perform our revenue estimations, and also to provide understanding for how reliant reddit and SO are on Wikipedia links. The second prevalence metric is the frequency ranking of Wikipedia as a domain on each platform - when grouping all links that appear into categories based on domain (i.e. all links to Youtube videos would fall under the "youtube.com domain"), the frequency rank is the position of a domain on an list of all domains sorted descendingly. So the most common domain has a frequency rank of 1, and the least common domain has a frequency rank equal to the total number of domains appearing on each platform.

We observe that on reddit, Wikipedia links comprise $VAR percent of all posts (including non-link posts, known as selfposts). However, Wikipedia links still have a frequency ranking of $VAR. When restricting analysis to only include the ten most subscribed-to subreddits [cite], we observe that Wikipedia as a domain has a frequency ranking of $VAR! We examined this particular subset because the the top ten most subscribed-to subreddits are more representative of the general traffic on reddit. On SO, Wikipedia links appear in $VAR percent of all answers, including answers with no links. They have a frequency ranking of $VAR. Notably, this number has actually decreased since the 2013 work of [somebody, et al] - at this time, Wikipedia links were the second most common type of external link.

When examining sampled data from reddit, we observe the following statistical differences between the treated group and the control group.
The treatment group had a mean score that was $VAR points higher (2x increase) and a mean number of comments that was $VAR comments higher ($VARx increase).
We also observe a higher median score, implying the difference in means is not the result of a few outliers (i.e. "superstar" Wikipedia links that perform exceptionally well on reddit). The median number of comments is the same, which implies that opposite - the distribution of comments may be more outlier-dependent ($NOTE: choose a better word for this).
On SO, the treatment group had a mean score that was $VAR points higher ($VARx increase) and a mean number of comments that was $VAR comments higher ($VARx increase). On this platform, we also observe a higher median score.

In answering RQ2, we observe the following statistical differences.
Both platforms have a similar distribution of links: $VAR percent of SO answers include "good" links on average, and $VAR percent of reddit posts include "good" links on average. This is ___ compared to the actual distribution of Wikipedia article (just $VAR percent are classified as Good or above).
On SO, the mean pageviews, score, and number of comments are all higher when an item includes good links ($VAR more pageviews, $VAR higher score, and $VAR more comments).
However, on reddit, the mean score and number of comments actually decreased when an item includes a good link.
We expand on implications in the Discussion section.

For RQ3, we observe a variety of new metrics.
After Wikipedia links are posted on reddit, the number of edits increases by about $VAR.