What about SO articles w/ multiple Wikipedia links?
    Sum the number of Wikipedia pageviews.
    Sum the number of Wikipedia edits.
    Pick the highest quality article for determining the "article quality".

Note that the textstat metrics are not used (sentence count and lexicon count)
    Don't work well with these weird posts. No research suggests they would be critical to include.




Known Issues:

SQL LIKE query doesn't have a LOWER(statement) - aren't you missing some Wikipedia links?
Reddit:
w/o LOWER(url) 121289
w LOWER(url) 121298

SO:
w/o LOWER(body): 291431
w LOWER(body): 291500



Other big sites?
SELECT
  avg(score)
FROM (
  SELECT
    *
  FROM
    TABLE_QUERY([fh-bigquery:reddit_posts], "REGEXP_MATCH(table_id, '^2016_..$')") )
WHERE
  domain like '%twitter.com'
reddit.com: 5.117333896826138
gfycat.com: 179.550638618397
%twitter.com: 53.66091580625132
%youtube.com: 16.944257757750076	
%wikipedia.org: 146.84111382767503
%imgur.com: 105.63193205844149
theguardian.com: 53.88428134281343
bbc.com: 58.335981036347

# TODO: run causal analysis for another domain for comparison... maybe twitter or youtube?



In the reported results, non-English WP articles were included in calcuations.
These make up a very small fraction of the total:
specifically, X% of SO and Y% of Reddit
These mainly caused two sources of error:
quality calculations
WP before and after calculations

Issue I hadn't considered: Articles in other languages with the same title as English WP articles

Conclusion: re-running all analyses with these posts explicitly exlcuded gives the same results.
This makes sense, considering they only make up 1-2% of WP links.